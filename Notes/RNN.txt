RNN
RNN stands for Recurrent Neural Network. It's a type of artificial neural network designed for sequence data, allowing information to persist within the network, making it suitable for tasks like natural language processing and time series analysis. RNNs have connections that form loops, enabling them to maintain a memory of previous inputs in their computations

Application
Natural Language Processing (NLP)
Time Series Prediction:
Image and Video Analysis:
Autonomous Vehicles:

Working
RNNs is to process sequential data while maintaining an internal memory or state that can capture information from previous inputs. This internal state allows RNNs to exhibit dynamic temporal behavior, making them powerful for tasks where the order of input data matters.

Types:
one-one - traditional architecture
one-many - production of music
many-one - sentiment analysis
many-many - langauge translation

hidden state
hidden state in an RNN carries information from previous time steps and serves as a memory mechanism. It helps the network maintain context and capture dependencies in sequential data

Backpropagation through time (BPTT)
BPTT is the algorithm used to train RNNs. It extends the backpropagation algorithm to handle sequences by unfolding the network through time. It essentially applies the standard backpropagation algorithm to the unrolled RNN over the entire sequence

Activation function
The activation function introduces non-linearity to the network, allowing it to learn complex patterns. Common activation functions used in RNNs include tanh and sigmoid.

Forward Propagation in RNN:
In forward propagation, an input sequence is processed through the recurrent neural network (RNN) from the initial time step to the final time step. At each time step, the network takes the input and the hidden state from the previous time step to produce an output and update the hidden state. This process is repeated for the entire sequence, and the final output is generated.

Backpropagation in RNN:
Backpropagation in RNN involves computing gradients of the loss with respect to the parameters (weights and biases) of the network. The backpropagation through time (BPTT) algorithm is used to handle sequences. Gradients are calculated at each time step, and the parameters are updated using an optimization algorithm like stochastic gradient descent. This iterative process is performed to train the RNN on sequential data, allowing it to learn and capture dependencies within the sequences.

Advantages of RNNs:
Handle sequential data effectively, including text, speech, and time series.
Process inputs of any length
Share weights across time steps, enhancing training efficiency

Disadvantages of RNNs:
Prone to vanishing and exploding gradient problems
Training can be challenging, especially for long sequences.
Computationally slower than other neural network architectures